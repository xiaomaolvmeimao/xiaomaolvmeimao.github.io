---
layout:     post
title:      Learning to Remember More with Less Memorization​
author:     Jexus
tags: 		NLP deep_learning slide
subtitle:   ICLR 2019 (Oral) paper - slide
category:  slideshare
---

## Learning to Remember More with Less Memorization​

[Paper Link](https://arxiv.org/abs/1901.01347)

### TL;DR

Memory-augmented Neural Network 如 Neural Turing Machine (NTM), differentiable neural computer (DNC) 雖然看起來很潮，但往往要 train 很久 or 天荒地老，而且表現有時候不一定較好，這篇 paper 透過讓 model 寫入寫出 memory 的次數規律化，減少了訓練時間，而表現反而變好，主要原因是減少了 model 無謂的寫入與寫出。

### Slide:

> Please wait a minute for the embedded frame to be displayed. Reading it on a computer screen is better.


<iframe src="https://onedrive.live.com/embed?cid=255C96F3631B0025&amp;resid=255C96F3631B0025%21447&amp;authkey=APH7PKzVX7ISLcs&amp;em=2&amp;wdAr=1.7777777777777777" width="962px" height="565px" frameborder="0">這是 <a target="_blank" href="https://office.com/webapps">Office</a> 提供的內嵌 <a target="_blank" href="https://office.com">Microsoft Office</a> 簡報。</iframe>